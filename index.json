
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Richard Schweitzer is a vision scientist currently working at the Berlin-based cluster of excellence \u0026#39;Science of intelligence\u0026#39;.\n","date":1646006400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646006400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Richard Schweitzer is a vision scientist currently working at the Berlin-based cluster of excellence 'Science of intelligence'.","tags":null,"title":"Richard Schweitzer","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://richardschweitzer.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"a974d186c98c66462b45285dbd86e1f1","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2022_eyetracking/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2022_eyetracking/","section":"publication","summary":"When analyzing eye tracking data, one of the central tasks is the detection of saccades. Although many automatic saccade detection algorithms exist, the field still debates how to deal with brief periods of instability around saccade offset, so-called post-saccadic oscillations (PSOs), which are especially prominent in today‚Äôs widely used video-based eye tracking techniques. There is good evidence that PSOs are caused by inertial forces that act on the elastic components of the eye, such as the iris or the lens. As this relative movement can greatly distort estimates of saccade metrics, especially saccade duration and peak velocity, video-based eye tracking has recurrently been considered unsuitable for measuring saccade kinematics. In this chapter, we review recent biophysical models that describe the relationship between pupil motion and eyeball motion. We found that these models were well capable of accurately reproducing saccade trajectories and we implemented a we framework for the simulation of saccades, PSOs, and fixations, which can be used -- just like datasets hand-labeled by human experts -- to evaluate detection algorithms and train statistical models. Moreover, as only pupil and corneal-reflection signals are observable in video-based eye tracking, one may also be able to use these models to predict the unobservable motion of the eyeball. Testing these predictions by analyzing saccade data that was registered with video-based and search-coil eye tracking techniques revealed strong relationships between the two types of measurements, especially when saccade offset is defined as the onset of the PSO. To enable eye tracking researchers to make use of this definition, we present and evaluate two novel algorithms -- one based on eye movement direction inversion and one based on linear classifiers previously trained on simulation data. These algorithms allow for the detection of PSO onset with high fidelity. Even though PSOs may still pose problems for a range of eye tracking applications, the techniques described here may help to alleviate these.","tags":["eye movements","video-based eye tracking","saccades","post-saccadic oscillations","saccade detection"],"title":"Definition, Modeling, and Detection of Saccades in the Face of Post-saccadic Oscillations","type":"publication"},{"authors":["Martin Rolfs","Richard Schweitzer"],"categories":null,"content":"","date":1643760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643760000,"objectID":"27768ca52ce0cdcf70cc35e04706be0f","permalink":"https://richardschweitzer.github.io/publication/rolfsschweitzer2022/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/rolfsschweitzer2022/","section":"publication","summary":"Researchers in the field of active perception study how sensory processes coalesce with motor actions to extract information from the world. Such actions intrinsically alter perceptual processing and have intended sensory outcomes, but also lead to incidental sensory consequences, which are side effects of moving the sensory surface to its intended goal. These incidental consequences of actions are generally considered a nuisance to perception that needs to be attenuated or suppressed during movement execution. In this Perspective, we propose instead that incidental sensory consequences of actions shape perceptual processes through action‚Äìperception couplings and we review evidence from the domain of active vision. We propose four hallmarks representing the degrees to which actions are an integral part of a perceptual processing architecture. Finally, we outline a research strategy for probing these hallmarks in active perceptual systems and conclude that researchers of perception should embrace the study of action kinematics in pursuit of their questions.","tags":["eye movements","intrasaccadic perception","motion streaks","object correspondence","secondary saccades"],"title":"Coupling perception to action through incidental sensory consequences of motor behaviour","type":"publication"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"521be6a80bd82ea81b8d7147fc3fb4e3","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2021_sciadv/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2021_sciadv/","section":"publication","summary":"Rapid eye movements (saccades) incessantly shift objects across the retina. To establish object correspondence, the visual system is thought to match surface features of objects across saccades. Here, we show that an object‚Äôs intrasaccadic retinal trace‚Äîa signal previously considered unavailable to visual processing‚Äîfacilitates this match making. Human observers made saccades to a cued target in a circular stimulus array. Using high-speed visual projection, we swiftly rotated this array during the eyes‚Äô flight, displaying continuous intrasaccadic target motion. Observers‚Äô saccades landed between the target and a distractor, prompting secondary saccades. Independently of the availability of object features, which we controlled tightly, target motion increased the rate and reduced the latency of gaze-correcting saccades to the initial presaccadic target, in particular when the target‚Äôs stimulus features incidentally gave rise to efficient motion streaks. These results suggest that intrasaccadic visual information informs the establishment of object correspondence and jump-starts gaze correction.","tags":["eye movements","intrasaccadic perception","motion streaks","object correspondence","secondary saccades"],"title":"Intrasaccadic motion streaks jump-start gaze correction","type":"publication"},{"authors":["Richard Schweitzer","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://richardschweitzer.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591142400,"objectID":"ff81a04609d29c6b4aa25ecb329d42b7","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2020_brm/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2020_brm/","section":"publication","summary":"To investigate visual perception around the time of eye movements, vision scientists manipulate stimuli contingent upon the onset of a saccade. For these experimental paradigms, timing is especially crucial, because saccade offset imposes a deadline on the display change. Although efficient online saccade detection can greatly improve timing, most algorithms rely on spatial-boundary techniques or absolute-velocity thresholds, which both suffer from weaknesses -- late detections and false alarms, respectively. We propose an adaptive, velocity-based algorithm for online saccade detection that surpasses both standard techniques in speed and accuracy and allows the user to freely define the detection criteria. Inspired by the Engbert‚ÄìKliegl algorithm for microsaccade detection, our algorithm computes two-dimensional velocity thresholds from variance in the preceding fixation samples, while compensating for noisy or missing data samples. An optional direction criterion limits detection to the instructed saccade direction, further increasing robustness. We validated the algorithm by simulating its performance on a large saccade dataset and found that high detection accuracy (false-alarm rates below 1%) could be achieved with detection latencies of only 3 ms. High accuracy was maintained even under simulated high-noise conditions. To demonstrate that purely intrasaccadic presentations are technically feasible, we devised an experimental test in which a Gabor patch drifted at saccadic peak velocities. Whereas this stimulus was invisible when presented during fixation, observers reliably detected it during saccades. Photodiode measurements verified that‚Äîincluding all system delays‚Äîthe stimuli were physically displayed on average 20 ms after saccade onset. Thus, the proposed algorithm provides a valuable tool for gaze-contingent paradigms.","tags":["saccade detection","eye movements","intrasaccadic perception","gaze-contingent presentation"],"title":"An adaptive algorithm for fast and reliable online saccade detection","type":"publication"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"47363dece20484d48cecdae11247b0ad","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2020_jov/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2020_jov/","section":"publication","summary":"When visual objects shift rapidly across the retina, they produce motion blur. Intra-saccadic visual signals, caused incessantly by our own saccades, are thought to be eliminated at early stages of visual processing. Here we investigate whether they are still available to the visual system and could in principle be used as cues for localizing objects as they change locations on the retina. Using a high-speed projection system, we developed a trans-saccadic identification task in which brief but continuous intra-saccadic object motion was key to successful performance. Observers made a saccade to a target stimulus that moved rapidly either up or down, strictly during the eye movement. Just as the target reached its final position, an identical distractor stimulus appeared on the opposite side, resulting in a display of two identical stimuli upon saccade landing. Observers had to identify the original target using the only available clue -- the target's intra-saccadic movement. In an additional replay condition, we presented the observers‚Äô own intra-saccadic retinal stimulus trajectories during fixation. Compared to the replay condition, task performance was impaired during saccades but recovered fully when a post-saccadic blank was introduced. Reverse regression analyses and confirmatory experiments showed that performance increased markedly when targets had long movement durations, low spatial frequencies, and orientations parallel to their retinal trajectory -- features that promote intra-saccadic motion streaks. Although the potential functional role of intra-saccadic visual signals is still unclear, our results suggest that they could provide cues to tracking objects that rapidly change locations across saccades.","tags":["eye movements","intrasaccadic perception","motion streaks","object correspondence"],"title":"Intra-saccadic motion streaks as cues to linking object locations across saccades","type":"publication"},{"authors":["Richard Schweitzer","Tamara Watson","John Watson","Martin Rolfs"],"categories":null,"content":"Everyone knows this nice party trick: Try to observe your own rapid eye movements (so-called saccades) in the mirror ‚Äì and realize that you will not be able to. Despite this striking example, we are not blind during saccades. This paper features a demonstration (as well as schematics and code to built it yourself) that is capable of producing highly salient, nicely resolvable stimuli which can only be perceived during saccades!\n","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"ec9e21b04b9541ad130ddc4e2d80e04e","permalink":"https://richardschweitzer.github.io/publication/schweitzerwatsonwatsonrolfs2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/schweitzerwatsonwatsonrolfs2019/","section":"publication","summary":"Everyone knows this nice party trick -- try to observe your own rapid eye movements (so-called saccades) in the mirror and realize that you will not be able to. Despite this striking example, we are not blind during saccades. This paper features a demonstration (as well as schematics and code to built it yourself) that is capable of producing highly salient, nicely resolvable stimuli which can only be perceived during saccades!","tags":["eye movements","anorthoscopic presentation","intrasaccadic perception","visual persistence","retinal painting"],"title":"The Joy of Retinal Painting: A Build-It-Yourself Device for Intrasaccadic Presentations","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://richardschweitzer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Tarryn Balsdon","Richard Schweitzer","Tamara Watson","Martin Rolfs"],"categories":null,"content":"When objects rapidly shift across the retina during saccades, they produce so-called motion streaks ‚Äì elongated traces of the stimulus trajectory. During natural vision, however, we rarely notice this type of smearing. Previous studies have shown that the mere presence of stimulus after the saccade can achieve this ‚Äòsaccadic omission‚Äô. Tarryn‚Äôs study investigates not only the time course of this process but also the unexpected role of distractor stimuli. She has written a nice piece on her work for Science Trends.\n","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526688000,"objectID":"5d6dd694dd913b700fed07d06f08f4cf","permalink":"https://richardschweitzer.github.io/publication/balsdonschweitzerwatsonrolfs2018/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/balsdonschweitzerwatsonrolfs2018/","section":"publication","summary":"When objects rapidly shift across the retina during saccades, they produce so-called motion streaks -- elongated traces of the stimulus trajectory. During natural vision, however, we rarely notice this type of smearing. Previous studies have shown that the mere presence of stimulus after the saccade can achieve this 'saccadic omission'. Tarryn's study investigates not only the time course of this process but also the unexpected role of distractor stimuli. She has written a nice piece on her work for {{\u003c staticref \"https://sciencetrends.com/gaps-in-perception-how-we-see-a-stable-world-through-moving-eyes/\" \"newtab\" \u003e}}Science Trends{{\u003c /staticref \u003e}}.","tags":["eye movements","saccadic omission","intra-saccadic perception","backward masking"],"title":"All is not lost: Post-saccadic contributions to the perceptual omission of intra-saccadic streaks","type":"publication"},{"authors":["Richard Schweitzer","Sabrina Trapp","Moshe Bar"],"categories":null,"content":"The temporal oddball effect ‚Äì that is, the phenomenon that the duration of an oddball stimulus is overestimated when compared to the duration of a standard stimulus which is repeatedly presented in a stream ‚Äì is thought to be driven by prediction errors. Suprisingly, and in contrast to this predominant hypothesis, we found that a more predictable oddball object (e.g., a pizza following a pizza cutter) is overestimated to a larger degree than a fully unpredictable oddball (e.g., a rubber duck following a pizza cutter). How could this be explained?\n","date":1484265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484265600,"objectID":"6dfa9777b7969d6057b895944cec8a3b","permalink":"https://richardschweitzer.github.io/publication/bartrappschweitzer2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bartrappschweitzer2017/","section":"publication","summary":"The temporal oddball effect -- that is, the phenomenon that the duration of an oddball stimulus is overestimated when compared to the duration of a standard stimulus which is repeatedly presented in a stream -- is thought to be driven by prediction errors. Suprisingly, and in contrast to this predominant hypothesis, we found that a more predictable oddball object (e.g., a pizza following a pizza cutter) is overestimated to a larger degree than a fully unpredictable oddball (e.g., a rubber duck following a pizza cutter). How could this be explained?","tags":["prediction","attention","time perception","association","context"],"title":"Associated information increases subjective perception of duration","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://richardschweitzer.github.io/project/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Psychophysics"],"title":"Time Perception","type":"project"}]