
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Richard Schweitzer is a vision scientist with a focus on computational models of the visual system which are empirically grounded in experiments using combinations of human psychophysics, eye and motion tracking, and M/EEG. He is currently assegnista di ricerca at the CIMeC where he works with Christoph Huber-Huber on the potential link between active vision and hippocampal mechanisms.\nDuring his PhD in Martin Rolfs\u0026#39; lab, he investigated the extent and the potential function of intra-saccadic vision. As a postdoc at the Berlin-based cluster of excellence Science of Intelligence he previously worked on the mechanisms underlying visual stability, not only in humans, but also in (custom-built) robots.\nIn his work he is passionate about applying novel technologies and paradigms, building his own research equipment, developing useful and publicly available methods and algorithms, and contributing to Open Science. He has more than 10 years of experience in experimental research, programming, data analysis and statistics, scientific communication and writing, supervision and teaching.\n","date":1750636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1750636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Richard Schweitzer is a vision scientist with a focus on computational models of the visual system which are empirically grounded in experiments using combinations of human psychophysics, eye and motion tracking, and M/EEG.","tags":null,"title":"Richard Schweitzer","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://richardschweitzer.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Richard Schweitzer","Mara Doering","Thomas Seel","Joerg Raisch","Martin Rolfs"],"categories":null,"content":"We rarely become aware of the immediate sensory consequences of our own saccades, that is, a massive amount of motion blur as the entire visual scene shifts across the retina. In this paper, we applied a novel tachistoscopic presentation technique to flash natural scenes in total darkness while observers made saccades. That way, motion smear induced by rapid image motion (otherwise omitted from perception) became readily observable. With this setup we could not only study the time course of motion smear generation and reduction, but also determine what visual features are encoded in smeared images. Low spatial frequencies and, most prominently, orientations parallel to the direction of the ongoing saccade. Using some cool computational modeling, we show that these results can be explained assuming no more than saccadic velocity and human contrast sensitivity profiles. To demonstrate that motion smear is directly linked to saccade dynamics, we show that the time course of perceived smear across observers can be predicted by a parsimonious motion-filter model that only takes the eyes’ trajectories as an input. And the best thing about it: This works even if no saccades are made and the visual consequences of saccades are merely replayed to the fixating eye! In the name of open science, all modeling code, as well as data and data analysis code, is again publicly available.\n","date":1750636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750636800,"objectID":"7626e80d7238633056585127ad6577b4","permalink":"https://richardschweitzer.github.io/publication/schweitzerdoeringseelraischrolfs2023/","publishdate":"2025-06-23T00:00:00Z","relpermalink":"/publication/schweitzerdoeringseelraischrolfs2023/","section":"publication","summary":"We rarely become aware of the immediate sensory consequences of our own saccades, that is, a massive amount of motion blur as the entire visual scene shifts across the retina. In this paper, we applied a novel tachistoscopic presentation technique to flash natural scenes in total darkness while observers made saccades. That way, motion smear induced by rapid image motion (otherwise omitted from perception) became readily observable. With this setup we could not only study the time course of motion smear generation and reduction, but also determine what visual features are encoded in smeared images. Low spatial frequencies and, most prominently, orientations parallel to the direction of the ongoing saccade. Using some cool computational modeling, we show that these results can be explained assuming no more than saccadic velocity and human contrast sensitivity profiles. To demonstrate that motion smear is directly linked to saccade dynamics, we show that the time course of perceived smear across observers can be predicted by a parsimonious motion-filter model that only takes the eyes' trajectories as an input. And the best thing is that this works even if no saccades are made and the visual consequences of saccades are merely replayed to the fixating eye. In the name of open science, all {{\u003c staticref \"https://github.com/richardschweitzer/MotionSmearModelingPlayground\" \"newtab\" \u003e}}modeling code{{\u003c /staticref \u003e}}, as well as data and {{\u003c staticref \"https://osf.io/bjgqk\" \"newtab\" \u003e}}data analysis code{{\u003c /staticref \u003e}}, is again publicly available.","tags":["eye movements","intrasaccadic perception","intrasaccadic smear","motion filter","tachistoscopic","natural scenes"],"title":"Saccadic omission revisited: What saccade-induced smear looks like","type":"publication"},{"authors":["Martin Rolfs","Richard Schweitzer","Eric Castet","Tamara Watson","Sven Ohl"],"categories":null,"content":"In this paper, we report a mysterious finding. When detecting rapid stimulus motion of a Gabor stimulus oriented orthogonal to its motion direction, it is not simply its absolute velocity that determines its visibility, but a combination of velocity and movement distance. Curiously, the specific combination that predicts velocity thresholds follows an oculomotor law - the main sequence, an exponential function describing the increase of saccadic velocity with growing amplitude. My proud contributions to this paper feature the masking experiment, the modeling of saccade trajectories which ultimately revealed significant correlations between saccade metrics and velocity thresholds, and most importantly, the early vision model to predict the measured psychophysical data - without fitting and based only on the trajectory of the stimulus. Finally, I evaluated the timing of the motion stimulus using photometric measurements using the LM03 lightmeter.\n","date":1746662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746662400,"objectID":"c774fa201a634ea076a9fdaa4f74c858","permalink":"https://richardschweitzer.github.io/publication/rolfsschweitzercastetwatsonohl2023/","publishdate":"2025-05-08T00:00:00Z","relpermalink":"/publication/rolfsschweitzercastetwatsonohl2023/","section":"publication","summary":"In this paper, we report a mysterious finding. When detecting rapid stimulus motion of a Gabor stimulus oriented orthogonal to its motion direction, it is not simply its absolute velocity that determines its visibility, but a combination of velocity and movement distance. Curiously, the specific combination that predicts velocity thresholds follows an oculomotor law, that is, the main sequence, an exponential function describing the increase of saccadic velocity with growing amplitude. My proud contributions to this paper feature the {{\u003c staticref \"https://osf.io/kvtsu\" \"newtab\" \u003e}}masking experiment{{\u003c /staticref \u003e}}, the {{\u003c staticref \"https://github.com/richardschweitzer/PostsaccadicOscillations\" \"newtab\" \u003e}}modeling of saccade trajectories{{\u003c /staticref \u003e}} which ultimately revealed significant correlations between saccade metrics and velocity thresholds, and most importantly, the {{\u003c staticref \"https://github.com/richardschweitzer/ModelingVisibilityOfSaccadelikeMotion\" \"newtab\" \u003e}}early vision model{{\u003c /staticref \u003e}} to predict the measured psychophysical data, without fitting and based only on the trajectory of the stimulus. Finally, I evaluated the timing of the motion stimulus using photometric measurements using the {{\u003c staticref \"https://github.com/richardschweitzer/LM03_lightmeter\" \"newtab\" \u003e}}LM03 lightmeter{{\u003c /staticref \u003e}}.","tags":["eye movements","sensorimotor contingency","main sequence","motion"],"title":"Lawful kinematics link eye movements to the limits of high-speed perception","type":"publication"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"When looking at data recorded by video-based eye tracking systems, one might have noticed brief periods of instability around saccade offset. These so-called post-saccadic oscillations are caused by inertial forces that act on the elastic components of the eye, such as the iris or the lens, and can greatly distort estimates of saccade duration and peak velocity. In this paper, we describe and evaluate biophysically plausible models (for a demonstration, see the shiny app) that can not only approximate saccade trajectories observed in video-based eye tracking, but also extract the underlying – and otherwise unobservable – rotation of the eyeball. We further present detection algorithms for post-saccadic oscillations, which are made publicly available, and finally demonstrate how accurate models of saccade trajectory can be used to generate data and mathematically tractable ground-truth labels for training ML-based algorithms that are capable of accurately detecting post-saccadic oscillations.\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"a974d186c98c66462b45285dbd86e1f1","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2022_eyetracking/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2022_eyetracking/","section":"publication","summary":"When looking at data recorded by video-based eye tracking systems, one might have noticed brief periods of instability around saccade offset. These so-called post-saccadic oscillations are caused by inertial forces that act on the elastic components of the eye, such as the iris or the lens, and can greatly distort estimates of saccade duration and peak velocity. In this paper, we describe and evaluate biophysically plausible models (for a demonstration, see the {{\u003c staticref \"https://richardschweitzer.shinyapps.io/pso_fitting_example/\" \"newtab\" \u003e}}shiny app{{\u003c /staticref \u003e}}) that can not only approximate saccade trajectories observed in video-based eye tracking, but also extract the underlying -- and otherwise unobservable -- rotation of the eyeball. We further present detection algorithms for post-saccadic oscillations, which are made {{\u003c staticref \"https://github.com/richardschweitzer/PostsaccadicOscillations\" \"newtab\" \u003e}}publicly available{{\u003c /staticref \u003e}}, and finally demonstrate how accurate models of saccade trajectory can be used to generate data and mathematically tractable ground-truth labels for training ML-based algorithms that are capable of accurately detecting post-saccadic oscillations.","tags":["eye movements","video-based eye tracking","saccades","post-saccadic oscillations","saccade detection"],"title":"Definition, Modeling, and Detection of Saccades in the Face of Post-saccadic Oscillations","type":"publication"},{"authors":["Martin Rolfs","Richard Schweitzer"],"categories":null,"content":"This perspective paper is dedicated to the question how an active perceptual system deals with the sensory consequences of its own actions. For instance, in the field of active vision little is known about the consequences of large-field smear induced by the rapid image shift caused by saccades. Whereas such information is thought to hinder visual processing, new evidence is discussed that sheds light on the intriguing possibility of action-perception couplings, that is, the idea that perception is shaped by sensory consequences of actions.\n","date":1643760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643760000,"objectID":"27768ca52ce0cdcf70cc35e04706be0f","permalink":"https://richardschweitzer.github.io/publication/rolfsschweitzer2022/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/rolfsschweitzer2022/","section":"publication","summary":"This perspective paper is dedicated to the question how an active perceptual system deals with the sensory consequences of its own actions. For instance, in the field of active vision little is known about the consequences of large-field smear induced by the rapid image shift caused by saccades. Whereas such information is thought to hinder visual processing, new evidence is discussed that sheds light on the intriguing possibility of action-perception couplings, that is, the idea that perception is shaped by sensory consequences of actions.","tags":["eye movements","intrasaccadic perception","sensorimotor contingency","active vision"],"title":"Coupling perception to action through incidental sensory consequences of motor behaviour","type":"publication"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"In this piece we showed that the visual traces that moving objects induce during saccades can facilitate secondary saccades in both accuracy and saccade initiation latency. Secondary saccades are typically prompted when one saccade does not entirely reach a target or when the saccade target is displaced in mid-flight. Our results provide evidence against the widely acknowledged notion that our brains preemptively discard visual information which reaches the eye during saccades. The paper has received some peer and media attention, such as a well-written commentary by Jasper Fabius and Stefan van der Stigchel, as well as articles in Nature Research Highlights, AAAS, New Scientist, or Vozpópuli (see the Rolfslab\u0026#39;s blog post for the full list). Notably, this study is the first one to apply the new TrackPixx eye tracking system, for which I have written a Matlab toolbox.\n","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"521be6a80bd82ea81b8d7147fc3fb4e3","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2021_sciadv/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2021_sciadv/","section":"publication","summary":"In this piece we showed that the visual traces that moving objects induce during saccades can facilitate secondary saccades in both accuracy and saccade initiation latency. Secondary saccades are typically prompted when one saccade does not entirely reach a target or when the saccade target is displaced in mid-flight. Our results provide evidence against the widely acknowledged notion that our brains preemptively discard visual information which reaches the eye during saccades. The paper has received some peer and media attention, such as a well-written {{\u003c staticref \"https://www.science.org/doi/10.1126/sciadv.abk0043\" \"newtab\" \u003e}}commentary by Jasper Fabius and Stefan van der Stigchel{{\u003c /staticref \u003e}}, as well as articles in {{\u003c staticref \"https://www.nature.com/articles/d41586-021-02058-9\" \"newtab\" \u003e}}Nature Research Highlights{{\u003c /staticref \u003e}}, {{\u003c staticref \"https://www.aaas.org/news/human-eyes-absorb-visual-information-even-during-rapid-eye-movements\" \"newtab\" \u003e}}AAAS{{\u003c /staticref \u003e}}, {{\u003c staticref \"https://www.newscientist.com/article/2285131-we-thought-our-eyes-turned-off-when-moving-quickly-but-thats-wrong/\" \"newtab\" \u003e}}New Scientist{{\u003c /staticref \u003e}}, or {{\u003c staticref \"https://www.vozpopuli.com/next/ojos-movimientos-sacadicos.html\" \"newtab\" \u003e}}Vozpópuli{{\u003c /staticref \u003e}} (see {{\u003c staticref \"https://rolfslab.org/2021/08/09/press-coverage-on-science-advances-paper/\" \"newtab\" \u003e}}the Rolfslab's blog post{{\u003c /staticref \u003e}} for the full list). Notably, this study is the first one to apply the new TrackPixx eye tracking system, for which I have written a {{\u003c staticref \"https://github.com/richardschweitzer/TrackPixxToolbox\" \"newtab\" \u003e}}Matlab toolbox{{\u003c /staticref \u003e}}.","tags":["eye movements","intrasaccadic perception","motion streaks","object correspondence","secondary saccades"],"title":"Intrasaccadic motion streaks jump-start gaze correction","type":"publication"},{"authors":["Richard Schweitzer"],"categories":null,"content":"Is intra-saccadic vision merely an epiphenomenon or could visual information that reaches the eye during saccades be used by the visual system? That was the question of my cumulative doctoral dissertation, which features not only a synopsis of all studies conducted up to this point, but also a review of the saccadic-suppression and motion-streak literature to put these findings into context. The dissertation has been awarded two prizes – the Humboldt Prize and the Lieselotte Pongratz-Promotionspreis by Studienstiftung des Deutschen Volkes (see also the short movie).\n","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607904000,"objectID":"b4c92e984d3b8ee51700d3b80e8a7e68","permalink":"https://richardschweitzer.github.io/publication/schweitzer2020_dissertation/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/schweitzer2020_dissertation/","section":"publication","summary":"Is intra-saccadic vision merely an epiphenomenon or could visual information that reaches the eye during saccades be used by the visual system? That was the question of my cumulative doctoral dissertation, which features not only a synopsis of all studies conducted up to this point, but also a review of the saccadic-suppression and motion-streak literature to put these findings into context. The dissertation has been awarded two prizes -- the {{\u003c staticref \"https://www.hu-berlin.de/de/ueberblick/menschen/ehrungen/humboldtpreis/archiv/humboldt-preis-2021/perceptual-and-motor-consequences-of-intra-saccadic-perception\" \"newtab\" \u003e}}Humboldt Prize{{\u003c /staticref \u003e}} and the {{\u003c staticref \"https://www.studienstiftung.de/pressemitteilungen/artikel/studienstiftung-vergibt-promotionspreise-2022-1/\" \"newtab\" \u003e}}Lieselotte Pongratz-Promotionspreis{{\u003c /staticref \u003e}} by Studienstiftung des Deutschen Volkes (see also the {{\u003c staticref \"https://youtu.be/Q6lAIwXfRVQ\" \"newtab\" \u003e}}short movie{{\u003c /staticref \u003e}}).","tags":["eye tracking","intrasaccadic perception","motion streaks","object correspondence","saccadic suppression","psychophysics"],"title":"Perceptual and Motor Consequences of Intra-saccadic Perception","type":"publication"},{"authors":["Richard Schweitzer","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://richardschweitzer.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"Whenever we make a saccade to an object, that object will travel from the periphery to the fovea at extremely high velocities. Depending on the visual features of the object, such motion can induce streaks, that may serve as visual clues to solve the problem of trans-saccadic object correspondence. Using a high-speed projection system operating at 1440 fps, we investigated to what extent human observers are capable of matching pre- and post-saccadic object locations when their only cue was an intra-saccadic motion streak, and compared their performance during saccades to a replay of the retinal stimulus trajectory presented during fixation.\nA toolbox for parsing Eyelink EDF files was implemented in R to analyze this series of experiments, which can be found here.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"47363dece20484d48cecdae11247b0ad","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2020_jov/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2020_jov/","section":"publication","summary":"Whenever we make a saccade to an object, that object will travel from the periphery to the fovea at extremely high velocities. Depending on the visual features of the object, such motion can induce streaks, that may serve as visual clues to solve the problem of trans-saccadic object correspondence. Using a high-speed projection system operating at 1440 fps, we investigated to what extent human observers are capable of matching pre- and post-saccadic object locations when their only cue was an intra-saccadic motion streak, and compared their performance during saccades to a replay of the retinal stimulus trajectory presented during fixation. Note that a toolbox for parsing Eyelink EDF files was implemented in R to analyze this series of experiments, which can be found {{\u003c staticref \"https://github.com/richardschweitzer/EyelinkEDFTools_R\" \"newtab\" \u003e}}here{{\u003c /staticref \u003e}}.","tags":["eye movements","intrasaccadic perception","motion streaks","object correspondence"],"title":"Intra-saccadic motion streaks as cues to linking object locations across saccades","type":"publication"},{"authors":["Richard Schweitzer","Martin Rolfs"],"categories":null,"content":"To study intrasaccadic vision, we need stimulus manipulations that occur strictly during saccades. Due to the brief durations of saccades, this can prove a difficult task, as various system latencies (eye tracker, refresh cycle, video delay, and some more) have to be considered. While most of these delays are hardware-dependent, one opportunity to alleviate timing issues in gaze-contingent eye-tracking paradigms is applying an efficient online saccade detection! In this paper we described such an algorithm, validated it in simulations and experiments, and made it publicly available) so that it can be used with a range of different programming languages.\n","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591142400,"objectID":"ff81a04609d29c6b4aa25ecb329d42b7","permalink":"https://richardschweitzer.github.io/publication/schweitzerrolfs2020_brm/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/schweitzerrolfs2020_brm/","section":"publication","summary":"To study intrasaccadic vision, we need stimulus manipulations that occur strictly during saccades. Due to the brief durations of saccades, this can prove a difficult task, as various system latencies (eye tracker, refresh cycle, video delay, and some more) have to be considered. While most of these delays are hardware-dependent, one opportunity to alleviate timing issues in gaze-contingent eye-tracking paradigms is applying an efficient online saccade detection! In this paper we described such an algorithm, validated it in simulations and experiments, and made it {{\u003c staticref \"https://github.com/richardschweitzer/OnlineSaccadeDetection\" \"newtab\" \u003e}}publicly available{{\u003c /staticref \u003e}} so that it can be used with a range of different programming languages.","tags":["saccade detection","eye movements","intrasaccadic perception","gaze-contingent presentation"],"title":"An adaptive algorithm for fast and reliable online saccade detection","type":"publication"},{"authors":["Richard Schweitzer","Tamara Watson","John Watson","Martin Rolfs"],"categories":null,"content":"Everyone knows this nice party trick: Try to observe your own rapid eye movements (so-called saccades) in the mirror – and realize that you will not be able to. Despite this striking example, we are not blind during saccades. This paper features a demonstration (as well as schematics and code to built it yourself) that is capable of producing highly salient, nicely resolvable stimuli which can only be perceived during saccades!\n","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"ec9e21b04b9541ad130ddc4e2d80e04e","permalink":"https://richardschweitzer.github.io/publication/schweitzerwatsonwatsonrolfs2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/schweitzerwatsonwatsonrolfs2019/","section":"publication","summary":"Everyone knows this nice party trick -- try to observe your own rapid eye movements (so-called saccades) in the mirror and realize that you will not be able to. Despite this striking example, we are not blind during saccades. This paper features a demonstration (as well as schematics and code to built it yourself) that is capable of producing highly salient, nicely resolvable stimuli which can only be perceived during saccades!","tags":["eye movements","anorthoscopic presentation","intrasaccadic perception","visual persistence","retinal painting"],"title":"The Joy of Retinal Painting: A Build-It-Yourself Device for Intrasaccadic Presentations","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://richardschweitzer.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Tarryn Balsdon","Richard Schweitzer","Tamara Watson","Martin Rolfs"],"categories":null,"content":"When objects rapidly shift across the retina during saccades, they produce so-called motion streaks – elongated traces of the stimulus trajectory. During natural vision, however, we rarely notice this type of smearing. Previous studies have shown that the mere presence of stimulus after the saccade can achieve this ‘saccadic omission’. Tarryn’s study investigates not only the time course of this process but also the unexpected role of distractor stimuli. She has written a nice piece on her work for Science Trends.\n","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526688000,"objectID":"5d6dd694dd913b700fed07d06f08f4cf","permalink":"https://richardschweitzer.github.io/publication/balsdonschweitzerwatsonrolfs2018/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/balsdonschweitzerwatsonrolfs2018/","section":"publication","summary":"When objects rapidly shift across the retina during saccades, they produce so-called motion streaks -- elongated traces of the stimulus trajectory. During natural vision, however, we rarely notice this type of smearing. Previous studies have shown that the mere presence of stimulus after the saccade can achieve this 'saccadic omission'. Tarryn's study investigates not only the time course of this process but also the unexpected role of distractor stimuli. She has written a nice piece on her work for {{\u003c staticref \"https://sciencetrends.com/gaps-in-perception-how-we-see-a-stable-world-through-moving-eyes/\" \"newtab\" \u003e}}Science Trends{{\u003c /staticref \u003e}}.","tags":["eye movements","saccadic omission","intra-saccadic perception","backward masking"],"title":"All is not lost: Post-saccadic contributions to the perceptual omission of intra-saccadic streaks","type":"publication"},{"authors":["Richard Schweitzer","Sabrina Trapp","Moshe Bar"],"categories":null,"content":"The temporal oddball effect – that is, the phenomenon that the duration of an oddball stimulus is overestimated when compared to the duration of a standard stimulus which is repeatedly presented in a stream – is thought to be driven by prediction errors. Suprisingly, and in contrast to this predominant hypothesis, we found that a more predictable oddball object (e.g., a pizza following a pizza cutter) is overestimated to a larger degree than a fully unpredictable oddball (e.g., a rubber duck following a pizza cutter). How could this be explained?\n","date":1484265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484265600,"objectID":"6dfa9777b7969d6057b895944cec8a3b","permalink":"https://richardschweitzer.github.io/publication/bartrappschweitzer2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bartrappschweitzer2017/","section":"publication","summary":"The temporal oddball effect -- that is, the phenomenon that the duration of an oddball stimulus is overestimated when compared to the duration of a standard stimulus which is repeatedly presented in a stream -- is thought to be driven by prediction errors. Suprisingly, and in contrast to this predominant hypothesis, we found that a more predictable oddball object (e.g., a pizza following a pizza cutter) is overestimated to a larger degree than a fully unpredictable oddball (e.g., a rubber duck following a pizza cutter). How could this be explained?","tags":["prediction","attention","time perception","association","context"],"title":"Associated information increases subjective perception of duration","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://richardschweitzer.github.io/project/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Psychophysics"],"title":"Time Perception","type":"project"}]